rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.8, vjust = 2, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
mu_rational
# Single-row data frame for the label
label_data_rational <- data.frame(
x = Inf,
y = Inf,
z = Inf,
label = paste("\nMu: ", mu_rational, "\nMean Tau: ", mean_tau_rational, "\nEmpiric Tau: ", empiric_tau_rational)
)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.8, vjust = 2, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, z=z, label = label),
hjust = 1.8, vjust = 2, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.1, vjust = 1.1, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.1, vjust = 1.1, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 5.1, vjust = 1.1, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# Create the data for the prior forest plot
y_ci_lower <- df_rational_all$par_n_coef - 1.96 * df_rational_all$par_n_se
y_ci_upper <- df_rational_all$par_n_coef + 1.96 * df_rational_all$par_n_se
# Prior estimates
prior_estimates <- data.frame(
task = df_rational_all$task,
type = "Prior",
estimate = df_rational_all$par_n_coef,
lower = y_ci_lower,
upper = y_ci_upper
)
# Posterior estimates
posterior_estimates <- data.frame(
task = df_rational_all$task,
type = "Posterior",
estimate = posterior_summary_df$mean,
lower = posterior_summary_df$lower_95,
upper = posterior_summary_df$upper_95
)
# Combine prior and posterior estimates
combined_estimates <- bind_rows(prior_estimates, posterior_estimates)
# Remove rows with NA in 'estimate' column
combined_estimates <- combined_estimates %>% filter(!is.na(estimate))
# Order by 'Prior' estimate
combined_estimates <- combined_estimates %>%
arrange(if_else(type == "Prior", estimate, NA_real_)) %>%
mutate(task = factor(task, levels = unique(task)))
# Single-row data frame for the label
label_data_rational <- data.frame(
x = Inf,
y = Inf,
label = paste("Mean Tau: ", mean_tau_rational, "\nEmpiric Tau: ", empiric_tau_rational)
)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.8, vjust = 14, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
View(cu_test_result)
# In this Script, we run a Multilevel Analysis of the Experiment data
# This script was created by Vincent on July 5nd 2024.
# You can choose the subset of data you want by selecting the number here:
subset <- 3 # Input value here
# 1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# 2: excluding those whose SD of decisions is 0
# 3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
#### Library Setup ####
# Local Path to Dropbox (change this to your local machine:)
dropbox_path <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
# Loading libraries:
library(remotes)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(estimatr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(msm)
library(multiwayvcov)
library(bayesmeta)
library(metafor)
library(forestplot)
library(gridExtra)
library(grid)
library(extraDistr)
library(tibble)
library(magrittr)
# Set seed
set.seed(123)
#### Dataset Setup ####
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw) %>%
filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
filter(!(par_n == ub_n & !is.na(ub_n))) %>%
filter(!(par_n == lb_n & !is.na(lb_n))) %>%
group_by(task) %>%
mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
ungroup()
View(data_unfiltered)
# In this Script, we run a Multilevel Analysis of the Experiment data
# This script was created by Vincent on July 5nd 2024.
# You can choose the subset of data you want by selecting the number here:
subset <- 1 # Input value here
# 1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# 2: excluding those whose SD of decisions is 0
# 3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
#### Library Setup ####
# Local Path to Dropbox (change this to your local machine:)
dropbox_path <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
# Loading libraries:
library(remotes)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(estimatr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(msm)
library(multiwayvcov)
library(bayesmeta)
library(metafor)
library(forestplot)
library(gridExtra)
library(grid)
library(extraDistr)
library(tibble)
library(magrittr)
# Set seed
set.seed(123)
#### Dataset Setup ####
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
filter(!(par_n == ub_n & !is.na(ub_n))) %>%
filter(!(par_n == lb_n & !is.na(lb_n))) %>%
group_by(task) %>%
mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
ungroup()
# In this Script, we run a Multilevel Analysis of the Experiment data
# This script was created by Vincent on July 5nd 2024.
# You can choose the subset of data you want by selecting the number here:
subset <- 1 # Input value here
# 1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# 2: excluding those whose SD of decisions is 0
# 3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
#### Library Setup ####
# Local Path to Dropbox (change this to your local machine:)
dropbox_path <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
# Loading libraries:
library(remotes)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(estimatr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(msm)
library(multiwayvcov)
library(bayesmeta)
library(metafor)
library(forestplot)
library(gridExtra)
library(grid)
library(extraDistr)
library(tibble)
library(magrittr)
# Set seed
set.seed(123)
#### Dataset Setup ####
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
filter(!(par_n == ub_n & !is.na(ub_n))) %>%
filter(!(par_n == lb_n & !is.na(lb_n))) %>%
group_by(task) %>%
mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
ungroup()
View(data_unfiltered)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw) %>%
filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
filter(!(par_n == ub_n & !is.na(ub_n))) %>%
filter(!(par_n == lb_n & !is.na(lb_n))) %>%
group_by(task) %>%
mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
ungroup()
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
filter(!(par_n == ub_n & !is.na(ub_n))) %>%
filter(!(par_n == lb_n & !is.na(lb_n))) %>%
group_by(task) %>%
mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
ungroup()
data_temp <- data_unfiltered %>%
dplyr::select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
dplyr::filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
dplyr::filter(!(par_n == ub_n & !is.na(ub_n))) %>%
dplyr::filter(!(par_n == lb_n & !is.na(lb_n))) %>%
dplyr::group_by(task) %>%
dplyr::mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
dplyr::ungroup()
View(data_temp)
# In this Script, we run a Multilevel Analysis of the Experiment data
# This script was created by Vincent on July 5nd 2024.
# You can choose the subset of data you want by selecting the number here:
subset <- 2 # Input value here
# 1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# 2: excluding those whose SD of decisions is 0
# 3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
#### Library Setup ####
# Local Path to Dropbox (change this to your local machine:)
dropbox_path <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
# Loading libraries:
library(remotes)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(estimatr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(msm)
library(multiwayvcov)
library(bayesmeta)
library(metafor)
library(forestplot)
library(gridExtra)
library(grid)
library(extraDistr)
library(tibble)
library(magrittr)
# Set seed
set.seed(123)
#### Dataset Setup ####
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
dplyr::select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
dplyr::filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
dplyr::filter(!(par_n == ub_n & !is.na(ub_n))) %>%
dplyr::filter(!(par_n == lb_n & !is.na(lb_n))) %>%
dplyr::group_by(task) %>%
dplyr::mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
dplyr::ungroup()
# Multiply cognitive uncertainty times 2 for those tasks that are binary and divide it by 100 for all tasks
data_temp <- data_temp %>%
mutate(cu = if_else(task %in% c("CHT", "PRD", "RIA", "VOT"), cu * 2, cu)) %>%
mutate(cu = cu / 100) # Divide cognitive uncertainty by 100
# Subset 1: save unrestricted dataset
data1 <- data_temp %>%
select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, ub_n_emp, lb_n_emp, dist_bound_n)
# In this Script, we run a Multilevel Analysis of the Experiment data
# This script was created by Vincent on July 5nd 2024.
# You can choose the subset of data you want by selecting the number here:
subset <- 2 # Input value here
# 1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# 2: excluding those whose SD of decisions is 0
# 3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
#### Library Setup ####
# Local Path to Dropbox (change this to your local machine:)
dropbox_path <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
# Loading libraries:
library(remotes)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(estimatr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(msm)
library(multiwayvcov)
library(bayesmeta)
library(metafor)
library(forestplot)
library(gridExtra)
library(grid)
library(extraDistr)
library(tibble)
library(magrittr)
# Set seed
set.seed(123)
#### Dataset Setup ####
# Read data:
data_unfiltered <- read.csv(file.path(dropbox_path, "main_run_analysis.csv"), header = TRUE)
# Keep only relevant columns and throw out all observations where par_n = upper or lower bound.
data_temp <- data_unfiltered %>%
dplyr::select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>%
dplyr::filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>%
dplyr::filter(!(par_n == ub_n & !is.na(ub_n))) %>%
dplyr::filter(!(par_n == lb_n & !is.na(lb_n))) %>%
dplyr::group_by(task) %>%
dplyr::mutate(
ub_n_emp = if_else(!is.na(ub_n), ub_n, max(par_n, na.rm = TRUE)),
lb_n_emp = if_else(!is.na(lb_n), lb_n, min(par_n, na.rm = TRUE))
) %>%
dplyr::ungroup()
# Multiply cognitive uncertainty times 2 for those tasks that are binary and divide it by 100 for all tasks
data_temp <- data_temp %>%
mutate(cu = if_else(task %in% c("CHT", "PRD", "RIA", "VOT"), cu * 2, cu)) %>%
mutate(cu = cu / 100) # Divide cognitive uncertainty by 100
# Subset 1: save unrestricted dataset
data1 <- data_temp
# Subset 2: exclude those with SD of decisions = 0
sd_by_respondent <- data_temp %>%
group_by(id, task) %>%
summarise(sd_par_n = sqrt(var(resp1_raw, na.rm = TRUE)))
data2 <- data_temp %>%
left_join(sd_by_respondent, by = c("id", "task")) %>%
filter(sd_par_n != 0) %>%
select(-sd_par_n)  # Optionally, remove the sd_par_n column
