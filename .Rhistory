t_values_SE_model2 <- rep(1, length(t_values_df_model2$t_value))
scale_parameter_t_model2 <- 10
# Run the bayesian meta-analysis for model2 t-values
bma_t_model2 <- bayesmeta(y = t_values_df_model2$t_value, sigma = t_values_SE_model2,
tau.prior = function (t) { dhalfnormal(t, scale = scale_parameter_t_model2) })
mean_tau_t_model2 <- round(bma_t_model2$summary["mean", "tau"], 2)
empiric_tau_t_model2 <- round(sqrt(var(t_values_df_model2$t_value)), 2)
# Initialize an empty dataframe to store posterior summaries for model2
t_posterior_summary_df_model2 <- data.frame(
Experiment = character(),
y = numeric(),
sigma = numeric(),
mode = numeric(),
median = numeric(),
mean = numeric(),
sd = numeric(),
lower_95 = numeric(),
upper_95 = numeric(),
stringsAsFactors = FALSE
)
# Loop through each experiment in sorted_t_values_labels_model2
for (experiment_label in sorted_t_values_labels_model2) {
# Check if the experiment label exists
if (experiment_label %in% sorted_t_values_labels_model2) {
label_index <- which(sorted_t_values_labels_model2 == experiment_label)
# Extract the posterior summary
summary_data <- bma_t_model2$theta[, label_index]
# Append the data to the dataframe
t_posterior_summary_df_model2 <- rbind(t_posterior_summary_df_model2, data.frame(
Experiment = experiment_label,
y = summary_data["y"],
sigma = summary_data["sigma"],
mode = summary_data["mode"],
median = summary_data["median"],
mean = summary_data["mean"],
sd = summary_data["sd"],
lower_95 = summary_data["95% lower"],
upper_95 = summary_data["95% upper"]
))
}
}
# Create the data for the prior forest plot for model2
y_ci_lower_model2 <- t_posterior_summary_df_model2$y - 1.96 * t_posterior_summary_df_model2$sigma
y_ci_upper_model2 <- t_posterior_summary_df_model2$y + 1.96 * t_posterior_summary_df_model2$sigma
# Prior estimates for model2
prior_estimates_t_model2 <- data.frame(
task = t_posterior_summary_df_model2$Experiment,
type = "Prior",
estimate = t_posterior_summary_df_model2$y,
lower = y_ci_lower_model2,
upper = y_ci_upper_model2
)
# Posterior estimates for model2
posterior_estimates_t_model2 <- data.frame(
task = t_posterior_summary_df_model2$Experiment,
type = "Posterior",
estimate = t_posterior_summary_df_model2$mean,
lower = t_posterior_summary_df_model2$lower_95,
upper = t_posterior_summary_df_model2$upper_95
)
# Combine prior and posterior estimates for model2
combined_estimates_t_model2 <- bind_rows(prior_estimates_t_model2, posterior_estimates_t_model2)
# Remove rows with NA in 'estimate' column
combined_estimates_t_model2 <- combined_estimates_t_model2 %>% filter(!is.na(estimate))
# Order by 'Prior' estimate
combined_estimates_t_model2 <- combined_estimates_t_model2 %>%
arrange(if_else(type == "Prior", estimate, NA_real_)) %>%
mutate(task = factor(task, levels = unique(task)))
# Single-row data frame for the label for model2
label_data_model2 <- data.frame(
x = -Inf,
y = Inf,
label = paste("Bayesian Tau: ", mean_tau_t_model2, "\nEmpiric Tau: ", empiric_tau_t_model2)
)
# Create the horizontal forest plot for model2 with confidence intervals
t_plot_model2 <- ggplot(combined_estimates_t_model2, aes(y = task, x = estimate, fill = type)) +
geom_col(position = position_dodge(width = 0.7), width = 0.6) +
geom_vline(xintercept = 0, linetype = "dashed", size = 0.3) +  # Add thicker line at 0
geom_vline(xintercept = -1.645, linetype = "dashed", size = 0.3, color = "black") +  # Add vertical line at -1.645 (10% significance level)
geom_vline(xintercept = -1.96, linetype = "dashed", size = 0.3, color = "black") +  # Add vertical line at -1.96 (5% significance level)
geom_vline(xintercept = -2.326, linetype = "dashed", size = 0.3, color = "black") +  # Add vertical line at -2.326 (2% significance level)
labs(
title = graph_title_t2,
x = "Estimate",
y = "Task",
fill = "Estimate Type"
) +
theme_minimal() +
theme(axis.text.y = element_text(angle = 0, hjust = 1, vjust = 0.0, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.box = "vertical",
legend.box.just = "left") +
scale_fill_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_model2, aes(x = x, y = y, label = label),
hjust = -0.1, vjust = 1.5, size = 3, color = "black", fill = "white", label.size = 0.5)
print(t_plot_model2)
####  Meta-Analysis for phi_e: ####
# Order the dataframe by the value of phi_e
phi_e_estimate <- phi_e_estimate[order(phi_e_estimate$Coefficient), ]
phi_e <- phi_e_estimate$Coefficient
phi_e_SE <- phi_e_estimate$SE
phi_e_labels <- phi_e_estimate$Task
scale_parameter_phi <- 100
empiric_tau_phi <- round(sqrt(var(phi_e_estimate$Coefficient)), 2)
# Run the bayesian meta-analysis
bma_phi_e <- bayesmeta (y=phi_e_estimate$Coefficient, sigma=phi_e_estimate$SE,
tau.prior=function (t) {dhalfnormal (t, scale=scale_parameter_phi) })
mean_tau_phi <- round(bma_phi_e$summary["mean", "tau"], 2)
# Initialize an empty dataframe to store posterior summaries
phi_e_posterior_summary_df <- data.frame(
Experiment = character(),
y = numeric(),
sigma = numeric(),
mode = numeric(),
median = numeric(),
mean = numeric(),
sd = numeric(),
lower_95 = numeric(),
upper_95 = numeric(),
stringsAsFactors = FALSE
)
# Loop through each experiment in sorted_phi_e_labels
for (experiment_label in phi_e_labels) {
# Check if the experiment label exists
if (experiment_label %in% phi_e_labels) {
label_index <- which(phi_e_labels == experiment_label)
# Extract the posterior summary
summary_data <- bma_phi_e$theta[, label_index]
# Append the data to the dataframe
phi_e_posterior_summary_df <- rbind(phi_e_posterior_summary_df, data.frame(
Experiment = experiment_label,
y = summary_data["y"],
sigma = summary_data["sigma"],
mode = summary_data["mode"],
median = summary_data["median"],
mean = summary_data["mean"],
sd = summary_data["sd"],
lower_95 = summary_data["95% lower"],
upper_95 = summary_data["95% upper"]
))
} else {
print(paste("Experiment label", experiment_label, "not found in the data."))
}
}
# Initialize an empty dataframe to store posterior summaries
phi_e_posterior_summary_df <- data.frame(
Experiment = character(),
y = numeric(),
sigma = numeric(),
mode = numeric(),
median = numeric(),
mean = numeric(),
sd = numeric(),
lower_95 = numeric(),
upper_95 = numeric(),
stringsAsFactors = FALSE
)
# Loop through each experiment in phi_e_labels
for (experiment_label in phi_e_labels) {
# Check if the experiment label exists
if (experiment_label %in% phi_e_labels) {
label_index <- which(phi_e_labels == experiment_label)
# Extract the posterior summary
summary_data <- bma_phi_e$theta[, label_index]
# Append the data to the dataframe
phi_e_posterior_summary_df <- rbind(phi_e_posterior_summary_df, data.frame(
Experiment = experiment_label,
y = summary_data["y"],
sigma = summary_data["sigma"],
mode = summary_data["mode"],
median = summary_data["median"],
mean = summary_data["mean"],
sd = summary_data["sd"],
lower_95 = summary_data["95% lower"],
upper_95 = summary_data["95% upper"]
))
}
}
# Create the data for the prior forest plot
y_ci_lower <- phi_e_posterior_summary_df$y - 1.96 * phi_e_posterior_summary_df$sigma
y_ci_upper <- phi_e_posterior_summary_df$y + 1.96 * phi_e_posterior_summary_df$sigma
# Prior estimates
prior_estimates_phi <- data.frame(
task = phi_e_posterior_summary_df$Experiment,
type = "Prior",
estimate = phi_e_posterior_summary_df$y,
lower = y_ci_lower,
upper = y_ci_upper
)
# Posterior estimates
posterior_estimates_phi <- data.frame(
task = phi_e_posterior_summary_df$Experiment,
type = "Posterior",
estimate = phi_e_posterior_summary_df$mean,
lower = phi_e_posterior_summary_df$lower_95,
upper = phi_e_posterior_summary_df$upper_95
)
# Combine prior and posterior estimates
combined_estimates_phi <- bind_rows(prior_estimates_phi, posterior_estimates_phi)
# Remove rows with NA in 'estimate' column
combined_estimates_phi <- combined_estimates_phi %>% filter(!is.na(estimate))
# Order by 'Prior' estimate
# Reverse order by 'Prior' estimate
combined_estimates_phi <- combined_estimates_phi %>%
arrange(if_else(type == "Prior", desc(estimate), NA_real_)) %>%
mutate(task = factor(task, levels = unique(task)))
# Single-row data frame for the label
label_data_phi <- data.frame(
x = Inf,
y = Inf,
label = paste("Bayesian Tau: ", mean_tau_phi, "\nEmpiric Tau: ", empiric_tau_phi)
)
# Single-row data frame for the label
label_data_phi <- data.frame(
x = Inf,
y = Inf,
label = paste("Bayesian Tau: ", mean_tau_phi, "\nEmpiric Tau: ", empiric_tau_phi)
)
# Create the horizontal forest plot with a thicker line at 0
phi_e_plot <- ggplot(combined_estimates_phi, aes(y = task, x = estimate, fill = type)) +
geom_col(position = position_dodge(width = 0.7), width = 0.6) +
geom_vline(xintercept = 0, linetype = "dashed", size = 0.3) +  # Add thicker line at 0
labs(
title = graph_title_phi,
x = "Estimate",
y = "Task",
fill = "Estimate Type"
) +
theme_minimal() +
theme(axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.box = "vertical",
legend.box.just = "left") +
scale_fill_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_phi, aes(x = x, y = y, label = label),
hjust = 1.1, vjust = 1.1, size = 3, color = "black", fill = "white", label.size = 0.5)
print(phi_e_plot)
#### Meta-Analysis for estimated vs. rational beta ####
# for now, import this dataframe -- later it should be produced in the same script:
load_path <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports/df_rational_all.csv"
df_rational_all <- read.csv(load_path)
# Order the dataframe by the value of 'par_n_coef'
df_rational_all <- df_rational_all[order(df_rational_all$par_n_coef), ]
scale_parameter_rational <- 0.1
# Create the bma_rational object
bma_rational <- bayesmeta(
y = df_rational_all$par_n_coef,
sigma = df_rational_all$par_n_se,
tau.prior = function(t) { dhalfnormal(t, scale = scale_parameter_rational) }  # Example prior, adjust as necessary
)
empiric_tau_rational <- round(sqrt(var(df_rational_all$par_n_coef)), 2)
mean_tau_rational <- round(bma_rational$summary["mean", "tau"], 2)
# Extract posterior summary
posterior_summary_df <- data.frame(
Task = df_rational_all$task,
y = bma_rational$theta["y", ],
sigma = bma_rational$theta["sigma", ],
mode = bma_rational$theta["mode", ],
median = bma_rational$theta["median", ],
mean = bma_rational$theta["mean", ],
sd = bma_rational$theta["sd", ],
lower_95 = bma_rational$theta["95% lower", ],
upper_95 = bma_rational$theta["95% upper", ]
)
# Create the data for the prior forest plot
y_ci_lower <- df_rational_all$par_n_coef - 1.96 * df_rational_all$par_n_se
y_ci_upper <- df_rational_all$par_n_coef + 1.96 * df_rational_all$par_n_se
# Prior estimates
prior_estimates <- data.frame(
task = df_rational_all$task,
type = "Prior",
estimate = df_rational_all$par_n_coef,
lower = y_ci_lower,
upper = y_ci_upper
)
# Posterior estimates
posterior_estimates <- data.frame(
task = df_rational_all$task,
type = "Posterior",
estimate = posterior_summary_df$mean,
lower = posterior_summary_df$lower_95,
upper = posterior_summary_df$upper_95
)
# Combine prior and posterior estimates
combined_estimates <- bind_rows(prior_estimates, posterior_estimates)
# Remove rows with NA in 'estimate' column
combined_estimates <- combined_estimates %>% filter(!is.na(estimate))
# Order by 'Prior' estimate
combined_estimates <- combined_estimates %>%
arrange(if_else(type == "Prior", estimate, NA_real_)) %>%
mutate(task = factor(task, levels = unique(task)))
# Single-row data frame for the label
label_data_rational <- data.frame(
x = Inf,
y = Inf,
label = paste("Bayesian Tau: ", mean_tau_rational, "\nEmpiric Tau: ", empiric_tau_rational)
)
# Create the horizontal forest plot with confidence intervals
rational_plot <- ggplot(combined_estimates, aes(y = task, x = estimate, color = type)) +
geom_point(position = position_dodge(width = 0.7), size = 3) +
geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, position = position_dodge(width = 0.7)) +  # Add confidence intervals
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 0
geom_vline(xintercept = 1, linetype = "dashed", color = "red", size = 0.3) +  # Add thicker red line at 1
labs(
title = "Coefficient plot: resp_n on par_n with 95% CI divided by rational beta",
x = "Estimate",
y = "Task",
color = "Estimate Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
axis.text.y = element_text(angle = 0, hjust = 1, margin = margin(r = 10)),  # Increase distance between experiments
panel.grid.major.y = element_blank(),   # Remove horizontal grid lines for clarity
panel.grid.minor.y = element_blank(),
legend.position = "bottom",
legend.justification = "left"
) +
scale_color_manual(values = c("Prior" = "cadetblue", "Posterior" = "brown2")) +  # Customize colors
geom_label(data = label_data_rational, aes(x = x, y = y, label = label),
hjust = 1.8, vjust = 14, size = 3, color = "black", fill = "white", label.size = 0.5)
print(rational_plot)
# SAve the plots:
# Save the plots
ggsave(save_path_t_plot, plot = t_plot, width = 10, height = 6)
ggsave(save_path_phi_e_plot, plot = phi_e_plot, width = 10, height = 6)
ggsave(save_path_t_plot_model2, plot = t_plot_model2, width = 10, height = 6)
ggsave(save_path_rational_plot, plot = rational_plot, width = 10, height = 6)
# In this Script, we run a Multilevel Analysis of the Experiment data
# The script is structured as follows:
# 0. Set file- (for input data) and save- paths.
PATH <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
path_data<- "Analyses/Data"
setwd(PATH)
savepath <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports" # This would need to be changed
subset <- 1 # I should do this later such that all models are estimated and I can pick which subset I want
# Subset==1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# Subset==2: excluding those whose SD of decisions is 0
# Subset==3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
# 1. Libraries. If the libraries are not installed, it will be installed automatically.
# When merging scripts, you can add the other required packages to the lists.
# 2. Dataset (this is structured such that it has the same name convention and code as sebastian's code)
# 3.1-3.5 This is the self-contained code that has no overlap with Sebastian
# and can not be reduced further. You probably want to transfer this directly to your script.
# In this Script, we run a Multilevel Analysis of the Experiment data
# The script is structured as follows:
# 0. Set file- (for input data) and save- paths.
PATH <- path.expand("~/Dropbox/Attenuation/Analyses/Data/")
path_data<- "Analyses/Data"
setwd(PATH)
savepath <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports" # This would need to be changed
subset <- 1 # I should do this later such that all models are estimated and I can pick which subset I want
# Subset==1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# Subset==2: excluding those whose SD of decisions is 0
# Subset==3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
# 1. Libraries. If the libraries are not installed, it will be installed automatically.
# When merging scripts, you can add the other required packages to the lists.
# 2. Dataset (this is structured such that it has the same name convention and code as sebastian's code)
# 3.1-3.5 This is the self-contained code that has no overlap with Sebastian
# and can not be reduced further. You probably want to transfer this directly to your script.
# Taken Directly from Sebastians code:
analysis_name <- "main_run"
path_input_analysis <- file.path(path_data,paste0(analysis_name,"_analysis", ".csv"))
df_all <- read.csv(path_input_analysis,  header = TRUE)
analysis_name <- "main_run"
path_input_analysis <- file.path(path_data, paste0(analysis_name, "_analysis", ".csv"))
path_input_analysis <- file.path(path_data,paste0(PATH, analysis_name,"_analysis", ".csv"))
df_all <- read.csv(path_input_analysis,  header = TRUE)
# In this Script, we run a Multilevel Analysis of the Experiment data
# The script is structured as follows:
# 0. Set file- (for input data) and save- paths.
PATH <- path.expand("~/Dropbox/Attenuation/")
path_data<- "Analyses/Data"
setwd(PATH)
savepath <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports" # This would need to be changed
subset <- 1 # I should do this later such that all models are estimated and I can pick which subset I want
# Subset==1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# Subset==2: excluding those whose SD of decisions is 0
# Subset==3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
# 1. Libraries. If the libraries are not installed, it will be installed automatically.
# When merging scripts, you can add the other required packages to the lists.
# 2. Dataset (this is structured such that it has the same name convention and code as sebastian's code)
# 3.1-3.5 This is the self-contained code that has no overlap with Sebastian
# and can not be reduced further. You probably want to transfer this directly to your script.
# 4. This code contains example usage for plots (for your illustration).
#### 1. Library Setup ####
# Loading libraries:
install_if_missing <- function(package) {
if (!require(package, character.only = TRUE)) {
install.packages(package, repos = "http://cran.us.r-project.org")
library(package, character.only = TRUE)
}
}
install_if_missing_remotes <- function(package) {
if (!require(package, character.only = TRUE)) {
remotes::install_github(package)
library(package, character.only = TRUE)
}
}
# List CRAN packages that are required
cran_packages <- c(
"dplyr", "ggplot2", "lmtest", "sandwich", "estimatr",
"tidyverse", "lme4", "lmerTest", "msm", "multiwayvcov",
"bayesmeta", "metafor", "forestplot", "gridExtra",
"grid", "extraDistr", "tibble", "magrittr"
)
# List GitHub packages that are required
github_packages <- c("remotes")
# Install and load CRAN packages
lapply(cran_packages, install_if_missing)
# Install and load GitHub packages
lapply(github_packages, install_if_missing_remotes)
#### 2. Dataset Setup ####
# Taken Directly from Sebastians code:
analysis_name <- "main_run"
path_input_analysis <- file.path(path_data,paste0(PATH, analysis_name,"_analysis", ".csv"))
df_all <- read.csv(path_input_analysis,  header = TRUE)
# In this Script, we run a Multilevel Analysis of the Experiment data
# The script is structured as follows:
# 0. Set file- (for input data) and save- paths.
PATH <- path.expand("~/Dropbox/Attenuation/")
path_data<- "Analyses/Data"
setwd(PATH)
savepath <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports" # This would need to be changed
subset <- 1 # I should do this later such that all models are estimated and I can pick which subset I want
# Subset==1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# Subset==2: excluding those whose SD of decisions is 0
# Subset==3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
# 1. Libraries. If the libraries are not installed, it will be installed automatically.
# When merging scripts, you can add the other required packages to the lists.
# 2. Dataset (this is structured such that it has the same name convention and code as sebastian's code)
# 3.1-3.5 This is the self-contained code that has no overlap with Sebastian
# and can not be reduced further. You probably want to transfer this directly to your script.
# 4. This code contains example usage for plots (for your illustration).
analysis_name <- "main_run"
path_input_analysis <- file.path(path_data,paste0(analysis_name,"_analysis", ".csv"))
df_all <- read.csv(path_input_analysis,  header = TRUE)
View(df_all)
# In this Script, we run a Multilevel Analysis of the Experiment data
# The script is structured as follows:
# 0. Set file- (for input data) and save- paths.
PATH <- path.expand("~/Dropbox/Attenuation/")
path_data<- "Analyses/Data"
setwd(PATH)
savepath <- "/Users/vincentmarohl/Desktop/Enke_predoc_R_exports" # This would need to be changed
subset <- 1 # I should do this later such that all models are estimated and I can pick which subset I want
# Subset==1: no subsetting other than the 6 outliers flagged by sebastian and those decisions at the lb and ub boundaries
# Subset==2: excluding those whose SD of decisions is 0
# Subset==3: excluding those for who the coefficient beta of resp_n on par_n is beta<= 0
# 1. Libraries. If the libraries are not installed, it will be installed automatically.
# When merging scripts, you can add the other required packages to the lists.
# 2. Dataset (this is structured such that it has the same name convention and code as sebastian's code)
# 3.1-3.5 This is the self-contained code that has no overlap with Sebastian
# and can not be reduced further. You probably want to transfer this directly to your script.
# 4. This code contains example usage for plots (for your illustration).
#### 1. Library Setup ####
# Loading libraries:
install_if_missing <- function(package) {
if (!require(package, character.only = TRUE)) {
install.packages(package, repos = "http://cran.us.r-project.org")
library(package, character.only = TRUE)
}
}
install_if_missing_remotes <- function(package) {
if (!require(package, character.only = TRUE)) {
remotes::install_github(package)
library(package, character.only = TRUE)
}
}
# List CRAN packages that are required
cran_packages <- c(
"dplyr", "ggplot2", "lmtest", "sandwich", "estimatr",
"tidyverse", "lme4", "lmerTest", "msm", "multiwayvcov",
"bayesmeta", "metafor", "forestplot", "gridExtra",
"grid", "extraDistr", "tibble", "magrittr"
)
# List GitHub packages that are required
github_packages <- c("remotes")
# Install and load CRAN packages
lapply(cran_packages, install_if_missing)
# Install and load GitHub packages
lapply(github_packages, install_if_missing_remotes)
#### 2. Dataset Setup ####
# Taken Directly from Sebastians code:
analysis_name <- "main_run"
path_input_analysis <- file.path(path_data,paste0(analysis_name,"_analysis", ".csv"))
df_all <- read.csv(path_input_analysis,  header = TRUE)
tasks_binary <- c("RIA",  "PRD",  "VOT", "CHT")
# Prepare df_all for meta analysis
df_temp <- df_all %>%
dplyr::select(id, task, resp_n, par_n, cu, FE, ub_n, lb_n, resp1_raw, dist_bound_n) %>% # Keep only relevant columns
dplyr::filter(!(task %in% c("BE1", "BE2", "BE3", "BE4", "BE5", "RIA"))) %>% # Filter out Pilot tasks and RIA
dplyr::mutate(cu = if_else(task %in% tasks_binary, cu * 2, cu)) %>% # Multiply CU in binary tasks by 2 to adjust
dplyr::mutate(cu = cu / 100) # Divide cognitive uncertainty by 100 (Ben's figures are in this scale)
View(df_temp)
# Subset 2: exclude those with SD of decisions = 0
temp_sd_by_respondent <- df_temp %>%
dplyr::group_by(id, task) %>%
dplyr::summarise(sd_par_n = sqrt(var(resp1_raw, na.rm = TRUE)))
df_temp2 <- df_temp %>%
dplyr::left_join(temp_sd_by_respondent, by = c("id", "task")) %>%
dplyr::filter(sd_par_n != 0) %>%
dplyr::select(-sd_par_n)  # Optionally, remove the sd_par_n column
# Subset 3: exclude those with SD of decisions = 0
temp_beta_by_respondent <- df_temp %>%
dplyr::group_by(id, task) %>%
dplyr::summarise(beta = {
model <- lm(par_n ~ resp_n)
coef(model)["resp_n"]
})
